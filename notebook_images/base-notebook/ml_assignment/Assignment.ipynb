{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SynergyLabs  Machine Learning Assignment\n",
    "\n",
    "### Overview\n",
    "You are expected to build a text classification model to predict positive and negative reviews (binary classification). The dataset you will use in this assignment contains text snippets from real-world reviews. We encourage you to actively explore novel ways of completing this assignment - there are no 'right\" or \"wrong\" answers. If not specified, you are free to set your own assumptions.\n",
    "\n",
    "If you are not comfortable with Jupyter Notebooks, please let us know before you start.\n",
    "\n",
    "The assignment is divided into three stages, each with different requirements. You will be building a simple machine learning pipeline that:\n",
    "\n",
    "* Read data points from text files\n",
    "* Featurize data points in a proposed way\n",
    "* Build a machine learning model based on the featurized data points\n",
    "\n",
    "Expected completion time: 5 hours\n",
    "\n",
    "Please talk to lab members (Sudershan, Chen, or Dohyun) if you have any question or doubt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset\n",
    "The `dataset` contains the following:\n",
    "* README.MD\n",
    "* stop_words.txt -- A textfile containing stop words with one stop word on every line (Source: [NLTK's list of english stopwords](https://gist.github.com/sebleier/554280))\n",
    "* reviews/ : Contains 2 directories (positive and negative). Each of these directories contain textfiles with reviews belonging to the class indicated by the directory name. (Source: [Movie Review Data](http://www.cs.cornell.edu/people/pabo/movie-review-data/))\n",
    "* Each textfile contains exactly one review, and each review is either positive or negative based on the directory that it belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: Tokenizing the text file\n",
    "\n",
    "> *There are a number of solutions to the given problem on websites like StackOverflow or through the use of open libraries (e.g. Sklearn.feature_extraction.text). However, you are expected **NOT** to copy & paste from them and **NOT** use pre-built libraries. You should complete this stage of the assignment using Python native libraries (e.g. open, readline, etc) only.*\n",
    "\n",
    "\n",
    "You can use [this reference](https://docs.python.org/3/tutorial/inputoutput.html) as a starting point.\n",
    "\n",
    "For each text file, you need to read the contents and transform it into a \"[Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model)\" representation. For example, if a review contains the following text:\n",
    "\n",
    "```John likes to watch movies. Mary likes movies too.```\n",
    "\n",
    "Then, we have one occurence of \"John\" and two occurences of \"movies\", etc. We count the number of times each word appears in the text file. If we transform the example review into its bag of words representation, it should look like the following.\n",
    "\n",
    "```BoW = {\"John\":1,\"likes\":2,\"to\":1,\"watch\":1,\"movies\":2,\"Mary\":1,\"too\":1}```\n",
    "\n",
    "When you build a bag of words, you can determine which word to count or not to count. You can consider not containing punctuations (e.g. \".\" or \"-\") or stop words (list of words that are not useful features - one such list is included in the archive you downloaded earlier).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Featurizing Data points\n",
    "> *There are a number of solutions to the given problem on websites like StackOverflow or through the use of open libraries (e.g. Sklearn.feature_extraction.text). However, you are expected **NOT** to copy & paste from them and **NOT** use pre-built libraries. You should complete this stage of the assignment using Python native libraries (e.g. open, readline, etc) only.*\n",
    "\n",
    "To extract better features out of reviews, we will use [TF/IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) featurization. There are different ways to get term frequency(TF) and inverse document frequency(IDF), please follow the same way as the example on Wikipedia link (\"Example of tf-idf\" section) for the sake of simplicity. You can also try additional techniques such as smoothing, but please do not spend too much time on those additional features.\n",
    "\n",
    "Implement and use TF/IDF to transform the Bag of Words into features for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 3: Build Machine Learning Models\n",
    "> *You **CAN** use external libraries for this stage.* *[Sklearn](http://scikit-learn.org/stable/index.html) and [Pandas](http://pandas.pydata.org/pandas-docs/stable/overview.html) are a good place to start and we highly encourage its use.*\n",
    "\n",
    "With data points featurized, you will now use it to build a machine learning model. You will need to build a two-label classification model using [Logistic Regression](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression). \n",
    "You are required to report test accuracy with your selected hyperparameter (only the C term). If a word that did not appear in the training set appears in the test set, you can skip the word. Also, please use IDF you learned from training set.\n",
    "\n",
    "You must decide the following for yourself:\n",
    "* How to represent each data point (vector, matrix, or sparse matrix, etc)\n",
    "* How to split the training/testing set\n",
    "* How to tune a hyper-parameter\n",
    "  * You do not have to exhaustively explore hyperparameters. It is sufficient to consider 3 different configurations for C (regularization penalty) of Logistic Regression. There are more hyper-parameters or configuration (like L1,L2 penalty), but it is okay if you want to use default values (e.g. L2 penalty) for other hyperparameters.\n",
    "\n",
    "**References:**\n",
    "* If you are a complete beginner to Machine Learning, start with [Introduction to ML](https://www.youtube.com/watch?v=nKW8Ndu7Mjw)\n",
    "* If you are not familiar with Logistic Regression specifically, start with [Logistic Regression](https://www.youtube.com/watch?v=ehGMpLeVgPs). You are welcome to watch the video from start to end, but we recommend skipping over some detail which are not essential to this assignment. We think that the following sections of the video should be sufficient for the assignment:\n",
    "  * [Introduction](https://www.youtube.com/embed/ehGMpLeVgPs?start=0&end=1068&version=3) - 0:00 to 17:48\n",
    "  * [Comparison to Linear Regression](https://www.youtube.com/embed/ehGMpLeVgPs?start=1500&end=1590&version=3) - 25:00 to 26:30\n",
    "  * [Regularization](https://www.youtube.com/embed/ehGMpLeVgPs?start=2270&end=3170&version=3) - 37:05 to 52:50\n",
    "  * [Logisitic Regression in Sklearn](https://www.youtube.com/embed/ehGMpLeVgPs?start=3178&version=3) - 52:58 to End\n",
    "* If you are unfamiliar with cross validation, refer to [Cross Validation](https://www.youtube.com/watch?v=sFO2ff-gTh0). \n",
    "* You can also refer to any source you can find on Internet, but just do not copy and paste any solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
